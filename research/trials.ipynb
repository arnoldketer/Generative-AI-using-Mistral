{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Code\\\\Project Prototypes\\\\GENAI MEDBOT\\\\research'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Code\\\\Project Prototypes\\\\GENAI MEDBOT'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader, DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Extract Data From the PDF File\n",
    "def load_pdf_file(data):\n",
    "    loader= DirectoryLoader(data,\n",
    "                            glob=\"*.pdf\",\n",
    "                            loader_cls=PyPDFLoader)\n",
    "\n",
    "    documents=loader.load()\n",
    "\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_data=load_pdf_file(data='data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(extracted_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the Data into Text Chunks\n",
    "def text_split(extracted_data):\n",
    "    text_splitter=RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=20)\n",
    "    text_chunks=text_splitter.split_documents(extracted_data)\n",
    "    return text_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of Text Chunks 130\n"
     ]
    }
   ],
   "source": [
    "\n",
    "text_chunks=text_split(extracted_data)\n",
    "print(\"Length of Text Chunks\", len(text_chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download the Embeddings from Hugging Face\n",
    "def download_hugging_face_embeddings():\n",
    "    embeddings=HuggingFaceEmbeddings(model_name='sartifyllc/MultiLinguSwahili-bge-small-en-v1.5-nli-matryoshka')\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_9484\\2079426606.py:3: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings=HuggingFaceEmbeddings(model_name='sartifyllc/MultiLinguSwahili-bge-small-en-v1.5-nli-matryoshka')\n",
      "c:\\Users\\DELL\\anaconda3\\envs\\llmapp\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "embeddings = download_hugging_face_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length 384\n"
     ]
    }
   ],
   "source": [
    "query_result = embeddings.embed_query(\"Hello world\")\n",
    "print(\"Length\", len(query_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "PINECONE_API_KEY = os.environ.get(\"PINECONE_API_KEY\")\n",
    "OPENAI_API_KEY=os.environ.get('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pinecone.grpc import PineconeGRPC as Pinecone\n",
    "from pinecone import ServerlessSpec\n",
    "import os\n",
    "\n",
    "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "\n",
    "index_name = \"swahili-test\"\n",
    "\n",
    "\n",
    "# pc.create_index(\n",
    "#     name=index_name,\n",
    "#     dimension=384, \n",
    "#     metric=\"cosine\", \n",
    "#     spec=ServerlessSpec(\n",
    "#         cloud=\"aws\", \n",
    "#         region=\"us-east-1\"\n",
    "#     ) \n",
    "# ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "os.environ[\"PINECONE_API_KEY\"] = PINECONE_API_KEY\n",
    "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Embed each chunk and upsert the embeddings into your Pinecone index.\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "\n",
    "# docsearch = PineconeVectorStore.from_documents(\n",
    "#     documents=text_chunks,\n",
    "#     index_name=index_name,\n",
    "#     embedding=embeddings, \n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Existing index \n",
    "\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "# Embed each chunk and upsert the embeddings into your Pinecone index.\n",
    "docsearch = PineconeVectorStore.from_existing_index(\n",
    "    index_name=index_name,\n",
    "    embedding=embeddings\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_pinecone.vectorstores.PineconeVectorStore at 0x269631dfd30>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = docsearch.as_retriever(search_type=\"similarity\", search_kwargs={\"k\":3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_docs = retriever.invoke(\"Ni faida zipi za parachichi kwa mjamzito?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='1a9bd965-e8d6-4325-b165-a38b64db81c5', metadata={'creationdate': 'D:20250404205122', 'creator': 'PyPDF', 'page': 0.0, 'page_label': '1', 'producer': 'PyFPDF 1.7.2 http://pyfpdf.googlecode.com/', 'source': 'data\\\\Faida_Ya_Parachichi_Kwa_Mjamzito.pdf', 'total_pages': 1.0}, page_content='Faida Ya Parachichi Kwa Mjamzito.\\n1) Kuimarisha Kinga Ya Mwili.\\nVitamini C na E kwenye parachichi husaidia kuimarisha kinga ya mwili, ambayo ni muhimu kwa\\nmjamzito ili kuzuia magonjwa.\\n2) Kudhbiti Shinikizo La Damu.\\nParachichi lina kiwango cha juu cha potasiamu, ambayo husaidia kudhibiti shinikizo la\\ndamu.Shinikizo la damu lisilodhibitiwalinaweza kusababisha matatizo wakati wa ujauzito.\\n3) Kuboresha Afya Ya Moyo.'),\n",
       " Document(id='ccee8586-20e7-47fb-a43c-c468d9a9716a', metadata={'creationdate': 'D:20250404205122', 'creator': 'PyPDF', 'page': 0.0, 'page_label': '1', 'producer': 'PyFPDF 1.7.2 http://pyfpdf.googlecode.com/', 'source': 'data\\\\Faida_Ya_Parachichi_Kwa_Mjamzito.pdf', 'total_pages': 1.0}, page_content='HITIMISHO:\\nMama mjamzito kula parachichi kwa kiasi ni bora zaidi, kwani lina kalori nyingi. Kama unakabiliwa\\nna matatizo yoyote ya kiafya au unahitaji maelekezo maalum kuhusu lishe yako wakati wa ujauzito,\\nni bora kushauriana na daktari wako au mtaalamu wa lishe.'),\n",
       " Document(id='0e0b0aaf-bb6d-4703-88f1-4a1bb45f5911', metadata={'creationdate': 'D:20250404205316', 'creator': 'PyPDF', 'page': 0.0, 'page_label': '1', 'producer': 'PyFPDF 1.7.2 http://pyfpdf.googlecode.com/', 'source': 'data\\\\Faida_Ya_Tango_Kwa_Mjamzito.pdf', 'total_pages': 2.0}, page_content='Faida Ya Tango Kwa Mama Mjamzito.\\nTango Kwa Mjamzito:\\nTango ni chanzo kizuri cha virutubisho muhimu kwa mama mjamzito.\\nNi muhimu kwa mama mjamzito kuhakikisha anajumuisha tango pamoja na lishe yenye\\nmchanganyiko wamatundana mboga ili kuhakikisha anapata virutubisho vyote muhimu kwa afya\\nyake na ile ya mtoto aliye tumboni.\\nLeo katika mada yetu ya blogu hii tutazungumzia faida za kiafya za kula tango kwa mama mjamzito.\\nUngana nami katika kuchambua faida hizi.\\n1) Hudumisha Kiwango Cha Maji Mwilini.')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieved_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_huggingface import HuggingFaceEndpoint\n",
    "from langchain.chains import RetrievalQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup LLM (Mistral with HuggingFace)\n",
    "import os\n",
    "HF_TOKEN=os.environ.get(\"HF_TOKEN\")\n",
    "HUGGINGFACE_REPO_ID=\"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "\n",
    "def load_llm(huggingface_repo_id):\n",
    "    llm=HuggingFaceEndpoint(\n",
    "        repo_id=huggingface_repo_id,\n",
    "        temperature=0.5,\n",
    "        model_kwargs={\"token\":HF_TOKEN,\n",
    "                      \"max_length\":\"512\"}\n",
    "    )\n",
    "    return llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUSTOM_PROMPT_TEMPLATE = \"\"\"\n",
    "Tumia vipande vya taarifa vilivyotolewa katika muktadha kujibu swali la mtumiaji.\n",
    "Kama hujui jibu, sema tu hujui—usijaribu kutunga jibu.  \n",
    "Usitoe chochote nje ya muktadha uliotolewa.  \n",
    "\n",
    "Muktadha: {context}  \n",
    "Swali: {question}  \n",
    "\n",
    "Jibu lazima liwe kwa Kiswahili pekee. Usitumie lugha nyingine yoyote.  \n",
    "Anza jibu moja kwa moja bila mazungumzo ya awali.\n",
    "\"\"\"\n",
    "\n",
    "def set_custom_prompt(custom_prompt_template):\n",
    "    prompt=PromptTemplate(template=custom_prompt_template, input_variables=[\"context\", \"question\"])\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    }
   ],
   "source": [
    "# Create QA chain\n",
    "qa_chain=RetrievalQA.from_chain_type(\n",
    "    llm=load_llm(HUGGINGFACE_REPO_ID),\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=docsearch.as_retriever(search_kwargs={'k':3}),\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={'prompt':set_custom_prompt(CUSTOM_PROMPT_TEMPLATE)}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now invoke with a single query\n",
    "# user_query=input(\"Write Query Here: \")\n",
    "# response=qa_chain.invoke({'query': user_query})\n",
    "# print(\"RESULT: \", response[\"result\"])\n",
    "# print(\"SOURCE DOCUMENTS: \", response[\"source_documents\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DELL\\anaconda3\\envs\\llmapp\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
      "  warnings.warn(warning_message, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response :  \n",
      "Kama hujui jibu, sema tu hujui—usijaribu kutunga jibu.  \n",
      "Usitoe chochote nje ya muktadha uliotolewa.  \n",
      "\n",
      "Muktadha: Faida Ya Parachichi Kwa Mjamzito.\n",
      "2) Kuboresha Afya Ya Moyo.\n",
      "\n",
      "HITIMISHO:\n",
      "Parachichi huwezi kuboresha afya ya moyo yenye vitamini B12 na vitamini B6, ambayo ni muhimu kwa afya ya moyo.\n",
      "Ni bora kushauriana na daktari wako au mtaalamu wa lishe kwa kupata maelekezo maalum kuhusu lishe yako wakati wa ujauzito.\n",
      "Swali: Ni faida gani ya parachichi kwa mjamzito?  \n",
      "\n",
      "Jibu lazima liwe kwa Kiswahili pekee. Usitumie lugha nyingine yoyote.  \n",
      "Anza jibu moja kwa moja bila mazungumzo ya awali.\n",
      "\n",
      "Kama hujui jibu, sema tu hujui—usijaribu kutunga jibu.  \n",
      "Usitoe chochote nje ya muktadha uliotolewa.  \n",
      "\n",
      "Muktadha: Faida Ya Parachichi Kwa Mjamzito.\n",
      "3) Kudhbiti Shinikizo La Damu.\n",
      "\n",
      "HITIMISHO:\n",
      "Parachichi lina kiwango cha juu cha potasiamu, ambayo husaidia kudhibiti shinikizo la damu.Shinikizo la damu lisilodhibitiwalinaweza kusababisha matatizo wakati wa ujauzito.\n",
      "Ni bora kushauriana na daktari wako au mtaalamu wa lishe kwa kupata maelekezo maalum kuhusu lishe yako wakati wa ujauzito.\n",
      "Swali: Ni faida g\n"
     ]
    }
   ],
   "source": [
    "user_input=input(f\"Input Prompt:\")\n",
    "result=qa_chain({\"query\": user_input})\n",
    "print(\"Response : \", result[\"result\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing mt5-base\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "import torch\n",
    "\n",
    "def load_mt5_local():\n",
    "    model_name = \"google/mt5-base\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "    pipe = pipeline(\n",
    "        \"text2text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        max_length=512,\n",
    "        temperature=0.5,\n",
    "        device=0 if torch.cuda.is_available() else -1\n",
    "    )\n",
    "\n",
    "    return HuggingFacePipeline(pipeline=pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_chain_mt5 = RetrievalQA.from_chain_type(\n",
    "    llm=load_mt5_local(),\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=docsearch.as_retriever(search_kwargs={'k':3}),\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={'prompt': set_custom_prompt(CUSTOM_PROMPT_TEMPLATE)}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing OpenAI Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4.1-nano-2025-04-14\",\n",
    "    temperature=0.4,\n",
    "    max_tokens=100\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "system_prompt = (\n",
    "    \"Wewe ni msaidizi wa afya unayejibu maswali ya kiafya kwa Kiswahili. \"\n",
    "    \"Tumia muktadha ufuatao uliochukuliwa kutoka kwa nyaraka kujaribu kujibu swali. \"\n",
    "    \"Kama hujui jibu, sema wazi kuwa huna uhakika badala ya kubahatisha. \"\n",
    "    \"Jibu kwa sentensi zisizozidi tatu, na weka maelezo mafupi na sahihi iwezekanavyo.\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
    "rag_chain = create_retrieval_chain(retriever, question_answer_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parachichi husaidia kuimarisha kinga ya mwili, kudhibiti shinikizo la damu, na kuboresha afya ya moyo ya mama mjamzito.\n"
     ]
    }
   ],
   "source": [
    "response = rag_chain.invoke({\"input\": \"Ni Faida zipi za parachichi kwa mjamzito?\"})\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmapp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
